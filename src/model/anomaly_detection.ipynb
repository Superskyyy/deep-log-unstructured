{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "anomaly_detection.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "TpcmiFCpS6xQ"
      ],
      "machine_shape": "hm"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# First enable parallel execution using TPU cores"
      ],
      "metadata": {
        "id": "IDM8RiW1Sc-N"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUD9TJsixI6K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20ede948-dd58-427d-fb3d-ed81d58db181"
      },
      "source": [
        "import tensorflow as tf\n",
        "use_tpu = True\n",
        "if use_tpu:\n",
        "    # Create distribution strategy\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.66.149.34:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.66.149.34:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.66.149.34:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.66.149.34:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import needed libs"
      ],
      "metadata": {
        "id": "WkkgktabSyKZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-_F7uwWUcCf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d122f78b-0fb8-4fa0-ee83-370642461863"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math, copy, time\n",
        "import pickle \n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from tqdm import trange\n",
        "import random\n",
        "import re\n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "import keras\n",
        "\n",
        "np.random.seed(0)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount dataset"
      ],
      "metadata": {
        "id": "oYtNm7lES3Ng"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME281tJIUOXa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8b57467-f81a-49af-aca1-40634dd133d2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# special to google colab | put your 4 Datasets into this path \n",
        "folder_path = 'drive/MyDrive/logsy_data/dataset'"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read in the datasets using preprocessor"
      ],
      "metadata": {
        "id": "TpcmiFCpS6xQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Standard tokenizer + do what the paper says\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "Do not run this script in Windows, it will not work due to use of signal\n",
        "\n",
        "We also use authors preprocessor to evaluate our model just in case we made something wrong here\n",
        "\"\"\"\n",
        "import os\n",
        "import pickle\n",
        "import re\n",
        "\n",
        "import signal\n",
        "\n",
        "\n",
        "class TimeoutException(Exception):  # Custom exception class\n",
        "    pass\n",
        "\n",
        "\n",
        "def timeout_handler(signum, frame):  # Custom signal handler\n",
        "    raise TimeoutException\n",
        "\n",
        "\n",
        "# Change the behavior of SIGALRM\n",
        "signal.signal(signal.SIGALRM, timeout_handler)\n",
        "\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "class DataTokenizer:\n",
        "    def __init__(self):\n",
        "        self.word2index = {'[PAD]': 0, '[CLS]': 1, '[MASK]': 2}\n",
        "        self.num_words = 3\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    def tokenize(self, message):\n",
        "        # paper section IV: Tokenization processing\n",
        "        message = message.lower()\n",
        "        message = re.sub(r'/.*:', '', message, flags=re.MULTILINE)  # filter for endpoints\n",
        "        message = re.sub(r'/.*', '', message, flags=re.MULTILINE)\n",
        "        message = word_tokenize(message)  # remove non words\n",
        "        message = (word for word in message if word.isalpha())  # generator  # remove numerical\n",
        "        message = [word for word in message if word not in self.stop_words]  # remove nltk common stopwords\n",
        "        message = ['[CLS]'] + message  # add embedding token\n",
        "        for word_idx, word in enumerate(message):  # convert to value\n",
        "            if word not in self.word2index:\n",
        "                self.word2index[word] = self.num_words\n",
        "                self.num_words += 1\n",
        "            message[word_idx] = self.word2index[word]\n",
        "        return message\n",
        "tokenizer = DataTokenizer()\n",
        "\n",
        "\n",
        "class DataImporter:\n",
        "    \"\"\"\n",
        "    loads data set from the raw dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, log_template, dataset_folder_path, dataset_name, dataset_step=1,\n",
        "                 dataset_limit=100000, dataset_type='main', normal_indicator: str = '-', aux_count=50000,\n",
        "                 chunk: bool = True):\n",
        "        self.log_template = log_template  # a template containing <Token{n}> and <Message>\n",
        "        self.log_dataframe = None\n",
        "        self.dataset_folder_path: str = dataset_folder_path  # path to the dataset folder\n",
        "        self.dataset_name: str = dataset_name  # full name of raw dataset\n",
        "        self.step: int = dataset_step  # step taken to sample auxiliary dataset\n",
        "        self.log_template_regex: re = re.compile(r'')\n",
        "        self.log_template_headers: list[str] = []\n",
        "        self.limit: int = dataset_limit  # used for faster experiment only\n",
        "        self.dataset_type: str = dataset_type\n",
        "        self.normal_indicator: str = normal_indicator  # a sign indicating the log line is anomaly\n",
        "        self.aux_count: int = aux_count\n",
        "        self.chunk: bool = chunk\n",
        "\n",
        "    def log_loader(self):\n",
        "        \"\"\"\n",
        "        read from IO stream and only take the actual log message based on template\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        log_messages = []\n",
        "        true_labels = []\n",
        "        with open(os.path.join(self.dataset_folder_path, self.dataset_name), 'r', encoding=\"latin-1\") as ds:\n",
        "            for line_no, line in enumerate(tqdm(ds, miniters=1)):\n",
        "                if line_no != 1 and line_no % 20000000 == 1:  # 15 file chunks\n",
        "                    with open(f'dataset/tbird/{self.dataset_name}_chunked_msg_line{line_no}', 'wb') as message_file:\n",
        "                        # print(log_messages)\n",
        "                        # log_messages_array = np.asanyarray(log_messages).reshape(-1, 1)\n",
        "                        print(len(log_messages))\n",
        "                        l = len(log_messages)\n",
        "                        tokenized = [tokenizer.tokenize(log_message) for log_message in\n",
        "                                     tqdm(log_messages, position=0, leave=True, total=l)]\n",
        "                        tokenized_np = np.asanyarray(tokenized)\n",
        "                        del tokenized\n",
        "                        pickle.dump(tokenized_np, message_file)\n",
        "                        log_messages = []  # reset\n",
        "                # Start the timer. Once 10 seconds are over, a SIGALRM signal is sent.\n",
        "                try:\n",
        "                    try:\n",
        "                        signal.alarm(30)\n",
        "\n",
        "                        match = self.log_template_regex.search(line.strip())\n",
        "\n",
        "                        if not match:\n",
        "                            continue\n",
        "                        label_decider = lambda x: 0 if x == self.normal_indicator else 1\n",
        "                        true_labels.append(label_decider(match.group('Token0')))\n",
        "                        # message = tokenizer.tokenize(match.group('Message'))\n",
        "                        # log_messages.append(message)\n",
        "                        log_messages.append(match.group('Message'))\n",
        "                        # print('message after tokenize ', message, log_messages)\n",
        "                        # print(self.log_template_headers)\n",
        "                    except Exception as e:  # noqa\n",
        "                        print(e)\n",
        "                        # print(e) # will skip those without normal indications('-' OR 'warn')\n",
        "                        pass\n",
        "                except TimeoutException:\n",
        "                    print(\"Regex hang detected, skipping\")\n",
        "                    continue  # catastrophic backtracking\n",
        "                else:\n",
        "                    signal.alarm(0)\n",
        "                if line_no == self.limit:\n",
        "                    break\n",
        "        return log_messages, np.array(true_labels)  # remaining log_messages and all of labels\n",
        "\n",
        "    def load(self):\n",
        "        self.log_template_matcher()\n",
        "\n",
        "        self.log_dataframe = self.log_loader()\n",
        "\n",
        "\n",
        "    def log_template_matcher(self):\n",
        "        headers = []\n",
        "        splitters = re.split(r'(<[^<>]+>)', self.log_template)\n",
        "        regex = ''\n",
        "        for k in range(len(splitters)):\n",
        "            if k % 2 == 0:\n",
        "                splitter = re.sub(' +', '\\\\\\s+', splitters[k])\n",
        "                regex += splitter\n",
        "            else:\n",
        "                header = splitters[k].strip('<').strip('>')\n",
        "                regex += '(?P<%s>.+?)' % header\n",
        "                headers.append(header)\n",
        "        print(regex)\n",
        "        regex = re.compile('^' + regex + '$')\n",
        "\n",
        "        self.log_template_headers, self.log_template_regex = headers, regex\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5b8wEE-yrWCB"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# original authors' data processor\n",
        "import numpy as np\n",
        "import math, copy, time\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import trange\n",
        "import re\n",
        "\n",
        "\n",
        "class LogReader:\n",
        "    def __init__(self, log_format, log_name, indir='./', outdir='./result/', rex=[], every_n=10, max_lines=2000000):\n",
        "    \n",
        "        self.path = indir\n",
        "       \n",
        "        self.logName = log_name\n",
        "        self.savePath = outdir\n",
        "        self.df_log = None\n",
        "        self.log_format = log_format\n",
        "        self.rex = rex\n",
        "        self.every_n = every_n\n",
        "        self.max_lines = max_lines\n",
        "    def log_to_dataframe(self, log_file, regex, headers, logformat):\n",
        "            \"\"\" Function to transform log file to dataframe \n",
        "            \"\"\"\n",
        "            log_messages = []\n",
        "            linecount = 0\n",
        "            \n",
        "            if self.max_lines:\n",
        "                with open(log_file, 'r', encoding=\"latin-1\") as fin:\n",
        "                    for i,  line in enumerate(fin):\n",
        "                        if i % self.every_n == 0:\n",
        "                            try:\n",
        "                                match = regex.search(line.strip())\n",
        "                                message = [match.group(header) for header in headers]\n",
        "                                log_messages.append(message)\n",
        "                                linecount += 1\n",
        "                            except Exception as e:\n",
        "                                pass\n",
        "                        if i==self.max_lines:\n",
        "                            break\n",
        "            else:\n",
        "                with open(log_file, 'r', encoding=\"latin-1\") as fin:\n",
        "                    for i,  line in enumerate(fin):\n",
        "                        if i % self.every_n == 0:\n",
        "                            try:\n",
        "                                match = regex.search(line.strip())\n",
        "                                message = [match.group(header) for header in headers]\n",
        "                                log_messages.append(message)\n",
        "                                linecount += 1\n",
        "                            except Exception as e:\n",
        "                                pass\n",
        "            logdf = pd.DataFrame(log_messages, columns=headers)\n",
        "            logdf.insert(0, 'LineId', None)\n",
        "            logdf['LineId'] = [i + 1 for i in range(linecount)]\n",
        "            return logdf\n",
        "\n",
        "\n",
        "    def generate_logformat_regex(self, logformat):\n",
        "        \"\"\" Function to generate regular expression to split log messages\n",
        "        \"\"\"\n",
        "        headers = []\n",
        "        splitters = re.split(r'(<[^<>]+>)', logformat)\n",
        "        regex = ''\n",
        "        for k in range(len(splitters)):\n",
        "            if k % 2 == 0:\n",
        "                splitter = re.sub(' +', '\\\\\\s+', splitters[k])\n",
        "                regex += splitter\n",
        "            else:\n",
        "                header = splitters[k].strip('<').strip('>')\n",
        "                regex += '(?P<%s>.*?)' % header\n",
        "                headers.append(header)\n",
        "        regex = re.compile('^' + regex + '$')\n",
        "        return headers, regex\n",
        "\n",
        "    def load_data(self):\n",
        "        headers, regex = self.generate_logformat_regex(self.log_format)\n",
        "        self.df_log = self.log_to_dataframe(os.path.join(self.path, self.logName), regex, headers, self.log_format)\n",
        "        \n",
        "        \n",
        "class LogTokenizer:\n",
        "    def __init__(self):\n",
        "        self.word2index = {'[PAD]':0, '[CLS]':1, '[MASK]':2}\n",
        "        self.index2word = {0:'[PAD]', 1:'[CLS]', 2:'[MASK]'}\n",
        "        self.n_words = 3  # Count SOS and EOS\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.regextokenizer =  nltk.RegexpTokenizer('\\w+|.|')\n",
        "        \n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "\n",
        "    def tokenize(self, sent):\n",
        "        sent = re.sub(r'\\/.*:', '', sent, flags=re.MULTILINE)\n",
        "        sent = re.sub(r'\\/.*', '', sent, flags=re.MULTILINE)\n",
        "        sent = self.regextokenizer.tokenize(sent)\n",
        "        sent = [w.lower() for w in sent]\n",
        "        sent = [word for word in sent if word.isalpha()]\n",
        "        sent = [w for w in sent if not w in self.stop_words]\n",
        "        sent = ['[CLS]'] + sent\n",
        "        for w in range(len(sent)):\n",
        "            self.addWord(sent[w])\n",
        "            sent[w] = self.word2index[sent[w]]\n",
        "        return sent\n",
        "    \n",
        "def get_data(log_file, input_dir, output_dir, log_format, regex=[], every_n=10, aux=0, max_lines=5000000):\n",
        "    reader = LogReader(log_format, log_file, indir=input_dir, outdir=output_dir, rex=regex, every_n=every_n, max_lines=max_lines)\n",
        "    reader.load_data()\n",
        "    log_payload, true_labels = reader.df_log.Content, np.where(reader.df_log.t.values=='-',0,1)\n",
        "    del reader\n",
        "    if aux != 0:\n",
        "        df_anomalies = log_payload.iloc[true_labels.flatten()==1].sample(n=aux).values\n",
        "        df_normal = log_payload.iloc[true_labels.flatten()==0].sample(n=aux).values\n",
        "        return df_normal, df_anomalies\n",
        "    else:\n",
        "        return log_payload, true_labels\n",
        "    \n",
        "def get_data_special(log_file, input_dir, output_dir, log_format, regex=[], every_n=10, aux=0, max_lines=2000000):\n",
        "    reader = LogReader(log_format, log_file, indir=input_dir, outdir=output_dir, rex=regex, every_n=every_n, max_lines=max_lines)\n",
        "    reader.load_data()\n",
        "    log_payload, true_labels = reader.df_log.Content, np.where(reader.df_log.t.values!='FATAL',0,1)\n",
        "    del reader\n",
        "\n",
        "    return log_payload, true_labels\n"
      ],
      "metadata": {
        "id": "QmysvQ57IyFV"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First 3 aux datasets - switch with target if you need to evaluate different target"
      ],
      "metadata": {
        "id": "SPy-ic9OS-MI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1scTBiN7QRSc"
      },
      "source": [
        "#read the datasets\n",
        "log_file = 'tbird2_small' # small meaning the dataset using only first 5m, note model doesn't converge well when sampling 5m instead of first 5m even if using original authors code\n",
        "input_dir  = folder_path\n",
        "log_format = '<t> <Timestamp> <Date> <User> <Month> <Day> <Time> <Location> <Component>(\\[<PID>\\])?: <Content>'  #thunderbird\n",
        "output_dir = '.'\n",
        "aux_normal, aux_anomalies = get_data(log_file, input_dir, output_dir, log_format, every_n=1, aux=200000, max_lines=False) # use 200000 when its tbird2 small (5m)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZr6kGpZUcCj"
      },
      "source": [
        "log_file = 'bgl2' # first 5 million, note bgl only has 5m when full dataset\n",
        "input_dir  = folder_path  # The input directory of log file\n",
        "output_dir = '.'  # The output directory of parsing results\n",
        "\n",
        "log_format = '<t> <Timestamp> <Date> <Node> <Time> <NodeRepeat> <Type> <Component> <Level> <Content>'  #BGL\n",
        "every_n = 1\n",
        "aux_size = 200000\n",
        "aux_normal1, aux_anomalies1 = get_data(log_file, input_dir, output_dir, log_format, every_n=1, aux=200000, max_lines=False)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdSU3380UcCk"
      },
      "source": [
        "log_file = 'Intrepid_RAS_0901_0908_scrubbed' # this one cannot be used as target, its too obvious\n",
        "input_dir  = folder_path\n",
        "output_dir = '.' \n",
        "log_format = '<f> <a>          <c>       <d>                  <e>    <t> <Content>'  #BGP\n",
        "aux_anomalies_t, tl = get_data_special(log_file, input_dir, output_dir, log_format, every_n=1, aux=False, max_lines=False)\n",
        "aux_normal2, aux_anomalies2  = aux_anomalies_t[tl==1], aux_anomalies_t[tl==0] # reverse this [TODO]"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Concat the auxiliary dataasets to one single piece, then sample 250000 when using 5m rows\n"
      ],
      "metadata": {
        "id": "oudsVRz7TL32"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taWdIBBLUcCl"
      },
      "source": [
        "# concatenate\n",
        "aux_anomalies = np.append(aux_anomalies, aux_anomalies1)\n",
        "aux_anomalies_full = np.append(aux_anomalies, aux_anomalies2)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a04tdsT-UcCl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ae49b4d-82ee-43e9-c1a7-5582b6ea1c03"
      },
      "source": [
        "# sample\n",
        "aux_anomalies = np.random.choice(aux_anomalies_full, size=500000, replace=False)  # changing size doesn't seem to change results\n",
        "print(f\"Selected {aux_anomalies.shape} samples out of {len(aux_anomalies_full)} aux samples\")"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected (500000,) samples out of 3310825 aux samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Then target dataset - switch with aux if you need to evaluate different target"
      ],
      "metadata": {
        "id": "CXwLwVukTIHs"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "LfyuPftFUcCj"
      },
      "source": [
        "log_file = 'spirit_small'\n",
        "input_dir  = folder_path\n",
        "output_dir = '.' \n",
        "log_format = '<t> <Timestamp> <Date> <User> <Month> <Day> <Time> <Location> <Content>'  #spirit2\n",
        "\n",
        "log_payload, true_labels = get_data(log_file, input_dir, output_dir, log_format, every_n=1, aux=False, max_lines=5000000)  # read at most 5m, else may crash"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIkTZFuQWuvq"
      },
      "source": [
        "### Never shuffle the data here, or there will be info leak"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4c-K9T7UcCm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9c1eaa6-e945-4c1e-fd22-6514836a4f04"
      },
      "source": [
        "df_size = len(log_payload)\n",
        "df_size"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4999991"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufdIgBLJUcCn"
      },
      "source": [
        "true_labels = true_labels.reshape(-1,1)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4z6oK4-TUcCn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d20a1b8e-464b-4037-8814-fa2358a54446"
      },
      "source": [
        "#append the anomalies to the full data\n",
        "log_payload = np.append(log_payload.values.reshape(-1,1), aux_anomalies.reshape(-1,1), axis=0)\n",
        "true_labels = np.append(true_labels, np.ones(len(aux_anomalies)).reshape(-1,1), axis=0).flatten()\n",
        "print(f'log shape {log_payload.shape}, log labels shape {true_labels.shape}')"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log shape (5499991, 1), log labels shape (5499991,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize!"
      ],
      "metadata": {
        "id": "MrQp5QZxXy-Z"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "3O2uU67VUcCo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d4ded1d-b42a-4ae3-bcbf-66ba44102554"
      },
      "source": [
        "tokenizer = LogTokenizer()\n",
        "df_len = int(log_payload.shape[0]) # the num of logs\n",
        "data_tokenized = []\n",
        "for i in trange(df_len):\n",
        "    tokenized = tokenizer.tokenize(log_payload[i][0])\n",
        "    data_tokenized.append(tokenized)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5499991/5499991 [02:17<00:00, 39995.16it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtWHQxFmRbTK"
      },
      "source": [
        "# pickle to file\n",
        "with open('tokenized','wb') as tokenized_file:\n",
        "  pickle.dump(data_tokenized, tokenized_file)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTfbilkfUl2q"
      },
      "source": [
        "with open('tokenized','rb') as token:\n",
        "  data_tokenized = pickle.load(token)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cWcsFTUUcCo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "036aae4e-2648-4750-f0e5-849e9a4996f9"
      },
      "source": [
        "data_token_indexed = np.asanyarray(data_tokenized)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order, subok=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORTANT: CHANGE TRAIN TEST SPLIT RATIO HERE"
      ],
      "metadata": {
        "id": "reKk7is8YJzk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhHKoqiAUcCo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63b88893-fed3-4fca-9b5d-876e150f2bf5"
      },
      "source": [
        "ratio = 0.6 # change from 0.1 0.2 0.4 0.6 0.8 its not always 0.8 performas better than 0.1\n",
        "train_size = round(df_size * ratio)\n",
        "print(df_size, train_size)\n",
        "print(f\"train size proportional to total logs - {train_size/df_size}\")"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4999991 2999995\n",
            "train size proportional to total logs - 0.600000080000144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2t94hDsUcCo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b955e338-478a-41fd-fbb6-2a84929d28e6"
      },
      "source": [
        "from collections import Counter\n",
        "# perform train test split\n",
        "\n",
        "# only take normal logs in target within ratio range \n",
        "data_token_indexed_train = np.append(data_token_indexed[:train_size][true_labels[:train_size]==0], data_token_indexed[df_size:],axis=0)\n",
        "\n",
        "# put remaining to test\n",
        "data_token_indexed_test = data_token_indexed[train_size:df_size]\n",
        "\n",
        "\n",
        "# labels using same split\n",
        "train_ground_labels = np.append(true_labels[:train_size][true_labels[:train_size]==0].flatten(), true_labels[df_size:].flatten(),axis=0)\n",
        "print(f\"train ground labels 0 1 counter {Counter(train_ground_labels)}\")\n",
        "\n",
        "test_ground_labels = true_labels[train_size:df_size] \n",
        "print(f\"test ground labels 0 1 counter {Counter(test_ground_labels)}\")\n",
        "\n",
        "print(data_token_indexed_test.shape, data_token_indexed_train.shape, train_ground_labels.shape, test_ground_labels.shape)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train ground labels 0 1 counter Counter({0.0: 2275123, 1.0: 500000})\n",
            "test ground labels 0 1 counter Counter({0.0: 1959978, 1.0: 40018})\n",
            "(1999996,) (2775123,) (2775123,) (1999996,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzjwoT7VUcCp"
      },
      "source": [
        "# verify unique log counts consistent with paper count\n",
        "# Commented because this line takes a long time to run\n",
        "# unique_counter = np.intersect1d(np.unique(data_token_indexed_test), np.unique(data_token_indexed_train), assume_unique=True)\n",
        "# print(f\"Unique log message count - {np.unique(data_token_indexed_test).shape[0] - unique_counter.shape[0]}\")"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5sxljzWUcCp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0a13780-15eb-4eae-aa0e-6f39e2cfaab9"
      },
      "source": [
        "# assign set names\n",
        "x_train = data_token_indexed_train\n",
        "y_train =  train_ground_labels\n",
        "x_test = data_token_indexed_test\n",
        "y_test = test_ground_labels\n",
        "\n",
        "# Add padding to sequences to maxlen 50 according to paper\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "x_train = pad_sequences(x_train, maxlen=50, truncating=\"post\", padding=\"post\") \n",
        "x_test = pad_sequences(x_test, maxlen=50, truncating=\"post\", padding=\"post\") \n",
        "print(x_train.shape, x_test.shape)\n",
        "print(x_train[0])\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
        "\n",
        "## padding masks\n",
        "x_train_masks = tf.equal(x_train, 0)\n",
        "x_test_masks = tf.equal(x_test, 0)\n",
        "print(x_train_masks,x_test_masks)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2775123, 50) (1999996, 50)\n",
            "[1 3 4 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "(2775123, 50) (2775123,) (1999996, 50) (1999996,)\n",
            "tf.Tensor(\n",
            "[[False False False ...  True  True  True]\n",
            " [False False False ...  True  True  True]\n",
            " [False False False ...  True  True  True]\n",
            " ...\n",
            " [False False False ...  True  True  True]\n",
            " [False False  True ...  True  True  True]\n",
            " [False False False ...  True  True  True]], shape=(2775123, 50), dtype=bool) tf.Tensor(\n",
            "[[False False False ...  True  True  True]\n",
            " [False False False ...  True  True  True]\n",
            " [False False False ...  True  True  True]\n",
            " ...\n",
            " [False False False ...  True  True  True]\n",
            " [False False False ...  True  True  True]\n",
            " [False False False ...  True  True  True]], shape=(1999996, 50), dtype=bool)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Standard transformer encoder with multi-head *attention*"
      ],
      "metadata": {
        "id": "PTrvJUVgaxWl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMyZcAlm9rKW"
      },
      "source": [
        "class TransformerEncoder(layers.Layer): # standard tensformer encoder\n",
        "    def __init__(self, embed_dim, num_heads, dense_dim, **kwargs):\n",
        "        super(TransformerEncoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim,dropout=0.1\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "        self.dropout1 = layers.Dropout(0.05)\n",
        "        self.dropout2 = layers.Dropout(0.05)\n",
        "\n",
        "    def call(self, inputs, training, mask=None):\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
        "        )\n",
        "        attention_output = self.dropout1(attention_output, training=training)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        proj_output = self.dropout2(proj_output, training=training)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics and model building"
      ],
      "metadata": {
        "id": "F8JtlyvubIxs"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy2gprEc9yeo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2b8b137-18b6-4fbe-c44b-75047a7e72a3"
      },
      "source": [
        "a = tf.metrics.BinaryAccuracy()\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    return true_positives / (possible_positives + K.epsilon())\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    return true_positives / (predicted_positives + K.epsilon())\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "def accuracy_m(y_true, y_pred):\n",
        "    a.update_state(y_true,y_pred)\n",
        "    return a.result()\n",
        "\n",
        "def create_model():\n",
        "  embed_dim = 16  # Embedding size for each token\n",
        "  num_heads = 2  # Number of attention heads\n",
        "  ff_dim = 16  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "  vocab_size = tokenizer.n_words  # Only consider the top 20k words\n",
        "  maxlen = 50\n",
        "\n",
        "  inputs = layers.Input(shape=(maxlen,))\n",
        "  embedding_layer = PositionalEmbedding(maxlen, vocab_size, embed_dim)\n",
        "  x = embedding_layer(inputs)\n",
        "  transformer_block = TransformerEncoder(embed_dim, num_heads, ff_dim)\n",
        "  transformer_block2 = TransformerEncoder(embed_dim, num_heads, ff_dim)\n",
        "\n",
        "  x = transformer_block(x)\n",
        "  x = transformer_block2(x)\n",
        "  \n",
        "  outputs = tf.gather(x, 0, axis=1) ## take the CLS token as embedding\n",
        "\n",
        "  function = K.sum(K.square(outputs),axis=1)\n",
        "  outputs = tf.keras.layers.Lambda(lambda x: K.sum(K.square(x),axis=1))(outputs)\n",
        "\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "  \n",
        "  return model\n",
        "\n",
        "# learning rate decay for optmimizer\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.0001, # 0.0001,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=1-0.001)\n",
        "\n",
        "# optimizer\n",
        "model_opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule,beta_1=0.9, beta_2=0.999)\n",
        "\n",
        "def custom_loss_function(y_true, y_pred):\n",
        "   loss = K.mean((1-y_true)*K.sqrt(y_pred) - (y_true)*K.log(1-K.exp(-K.sqrt(y_pred))))\n",
        "   #loss = K.mean((1-y_true)*K.square(dist) - (y_true)*K.log(1-K.exp(-K.square(dist))))\n",
        "\n",
        "   return loss\n",
        "\n",
        "# Create model with parallel execution - IMPORTANT!!\n",
        "with strategy.scope():\n",
        "  model = create_model()\n",
        "\n",
        "# NOTE! the metrics here don't make sense, it requires threshold derivation\n",
        "model.compile(optimizer=model_opt, loss=custom_loss_function, metrics=['binary_accuracy',recall_m,precision_m],loss_weights = [0.2, 1.0])  # this weight is [0.5, 1.0] in paper\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 50)]              0         \n",
            "                                                                 \n",
            " positional_embedding_3 (Pos  (None, 50, 16)           45264     \n",
            " itionalEmbedding)                                               \n",
            "                                                                 \n",
            " transformer_encoder_6 (Tran  (None, 50, 16)           2768      \n",
            " sformerEncoder)                                                 \n",
            "                                                                 \n",
            " transformer_encoder_7 (Tran  (None, 50, 16)           2768      \n",
            " sformerEncoder)                                                 \n",
            "                                                                 \n",
            " tf.compat.v1.gather_3 (TFOp  (None, 16)               0         \n",
            " Lambda)                                                         \n",
            "                                                                 \n",
            " lambda_3 (Lambda)           (None,)                   0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 50,800\n",
            "Trainable params: 50,800\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Threshold derivation! Evaluation step begins from fit"
      ],
      "metadata": {
        "id": "MQmLA2orbjYq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have no idea how original author got the thresholds, we use average of auc score"
      ],
      "metadata": {
        "id": "rcFFgkQCctd7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHXNHOPhOhN6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71d540f4-d10b-4b30-cfc4-fb30bc89104d"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def evaluation_threshold(threshold, y_test, distances):\n",
        "\n",
        "  tr = [threshold]\n",
        "\n",
        "  for t in tr:\n",
        "    res=[]\n",
        "    for d in distances:\n",
        "      if d > t:\n",
        "        res.append(1)\n",
        "      else:\n",
        "        res.append(0)\n",
        "\n",
        "    print('threshold value: ', t)\n",
        "    print('f1: ', f1_score(y_test.astype(np.int32), res))\n",
        "    print('recall: ',  recall_score(y_test.astype(np.int32), res))\n",
        "    print('precision: ', precision_score(y_test.astype(np.int32), res))\n",
        "    print('Accuracy: ', accuracy_score(y_test.astype(np.int32), res))\n",
        "\n",
        "    print('##################################')\n",
        "\n",
        "# sometimes it can be a bad run, just rerun to see if results improve\n",
        "for i in range(20):\n",
        "  print(f\"Currently Running Epoch - {i+1}\")\n",
        "  model.fit(x_train, y_train, batch_size=2048, epochs=1)\n",
        "\n",
        "  test_distances = model.predict(x_test,batch_size=2048)\n",
        "  test_auc = roc_auc_score(y_test.astype(np.int32), test_distances)\n",
        "\n",
        "  train_distances = model.predict(x_train,batch_size=2048)\n",
        "  train_auc = roc_auc_score(y_train.astype(np.int32), train_distances)\n",
        "\n",
        "  fpr_test, tpr_test, thresholds_test = roc_curve(y_test.astype(np.int32), test_distances, pos_label=1)\n",
        "  fpr_train, tpr_train, thresholds_train = roc_curve(y_train.astype(np.int32), train_distances, pos_label=1)\n",
        "\n",
        "  print(f'Train AUC: {train_auc}')\n",
        "  print(f'Test AUC: {test_auc}')\n",
        "\n",
        "  if train_auc == 1: # prevent calculation go wrong, not impacting results\n",
        "    train_auc = 0.99\n",
        "  if test_auc == 1:\n",
        "    test_auc = 0.99\n",
        "\n",
        "\n",
        "  print('Using trainset to find threshold:') # this should be legit\n",
        "  evaluation_threshold(np.average(thresholds_train[tpr_train>train_auc]), y_test, test_distances)\n",
        "  print('Using testset to find threshold:') # this is what the authors used, which leaks info\n",
        "  evaluation_threshold(np.average(thresholds_test[tpr_test>test_auc]), y_test, test_distances)\n",
        "\n"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Currently Running Epoch - 1\n",
            "1356/1356 [==============================] - 46s 26ms/step - loss: 0.5030 - binary_accuracy: 0.2359 - recall_m: 0.9999 - precision_m: 0.1919\n",
            "Train AUC: 0.9999505831544052\n",
            "Test AUC: 0.99674711777883\n",
            "Using trainset to find threshold:\n",
            "threshold value:  7.584181\n",
            "f1:  0.09255119470699871\n",
            "recall:  0.9999000449797592\n",
            "precision:  0.04852116783541558\n",
            "Accuracy:  0.6076687153374307\n",
            "##################################\n",
            "Using testset to find threshold:\n",
            "threshold value:  7.6317677\n",
            "f1:  0.1101564500188577\n",
            "recall:  0.9999000449797592\n",
            "precision:  0.05828900063512781\n",
            "Accuracy:  0.6767663535327071\n",
            "##################################\n",
            "Currently Running Epoch - 2\n",
            "1356/1356 [==============================] - 35s 26ms/step - loss: 0.3608 - binary_accuracy: 0.4288 - recall_m: 0.9995 - precision_m: 0.2400\n",
            "Train AUC: 0.9996841298786923\n",
            "Test AUC: 0.9915002078633108\n",
            "Using trainset to find threshold:\n",
            "threshold value:  4.5026674\n",
            "f1:  0.12521734231238135\n",
            "recall:  0.9996751461842172\n",
            "precision:  0.0667917742853756\n",
            "Accuracy:  0.720519941039882\n",
            "##################################\n",
            "Using testset to find threshold:\n",
            "threshold value:  4.5110335\n",
            "f1:  0.14957852171328262\n",
            "recall:  0.9996751461842172\n",
            "precision:  0.08083696212251332\n",
            "Accuracy:  0.7725530451060902\n",
            "##################################\n",
            "Currently Running Epoch - 3\n",
            "1356/1356 [==============================] - 33s 24ms/step - loss: 0.2888 - binary_accuracy: 0.4708 - recall_m: 0.9991 - precision_m: 0.2539\n",
            "Train AUC: 0.997554404799213\n",
            "Test AUC: 0.6854725109957762\n",
            "Using trainset to find threshold:\n",
            "threshold value:  2.239976\n",
            "f1:  0.03968293341165078\n",
            "recall:  0.9994252586336149\n",
            "precision:  0.020243355553193537\n",
            "Accuracy:  0.03212906425812852\n",
            "##################################\n",
            "Using testset to find threshold:\n",
            "threshold value:  2.2464345\n",
            "f1:  0.039842403594234084\n",
            "recall:  0.9994252586336149\n",
            "precision:  0.02032636018137446\n",
            "Accuracy:  0.03616307232614465\n",
            "##################################\n",
            "Currently Running Epoch - 4\n",
            "1356/1356 [==============================] - 32s 24ms/step - loss: 0.2201 - binary_accuracy: 0.4733 - recall_m: 0.9989 - precision_m: 0.2548\n",
            "Train AUC: 0.9975150865109271\n",
            "Test AUC: 0.6750533702982509\n",
            "Using trainset to find threshold:\n",
            "threshold value:  0.7634138\n",
            "f1:  0.051371562097535024\n",
            "recall:  0.9987505622469889\n",
            "precision:  0.0263638030683013\n",
            "Accuracy:  0.2619485238970478\n",
            "##################################\n",
            "Using testset to find threshold:\n",
            "threshold value:  0.76108146\n",
            "f1:  0.04586432558192841\n",
            "recall:  1.0\n",
            "precision:  0.023470389585711085\n",
            "Accuracy:  0.16748633497266996\n",
            "##################################\n",
            "Currently Running Epoch - 5\n",
            "1356/1356 [==============================] - 32s 24ms/step - loss: 0.1245 - binary_accuracy: 0.6593 - recall_m: 0.9962 - precision_m: 0.5050\n",
            "Train AUC: 0.9975554228162609\n",
            "Test AUC: 0.9693365029356665\n",
            "Using trainset to find threshold:\n",
            "threshold value:  0.00030399614\n",
            "f1:  0.041059915895814224\n",
            "recall:  1.0\n",
            "precision:  0.020960271439129156\n",
            "Accuracy:  0.06539163078326157\n",
            "##################################\n",
            "Using testset to find threshold:\n",
            "threshold value:  0.00039015716\n",
            "f1:  0.10683531935399043\n",
            "recall:  1.0\n",
            "precision:  0.05643213210460632\n",
            "Accuracy:  0.6654408308816617\n",
            "##################################\n",
            "Currently Running Epoch - 6\n",
            "1356/1356 [==============================] - 33s 24ms/step - loss: 0.0247 - binary_accuracy: 0.9992 - recall_m: 0.9953 - precision_m: 0.9998\n",
            "Train AUC: 0.9999832062890666\n",
            "Test AUC: 0.6051871265370056\n",
            "Using trainset to find threshold:\n",
            "threshold value:  0.0003215018\n",
            "f1:  0.30486353140283695\n",
            "recall:  0.531160977560098\n",
            "precision:  0.21378283783240135\n",
            "Accuracy:  0.9515329030658062\n",
            "##################################\n",
            "Using testset to find threshold:\n",
            "threshold value:  0.00025931795\n",
            "f1:  0.03925848570628339\n",
            "recall:  1.0\n",
            "precision:  0.020022264750397138\n",
            "Accuracy:  0.020669541339082678\n",
            "##################################\n",
            "Currently Running Epoch - 7\n",
            "1356/1356 [==============================] - 32s 23ms/step - loss: 0.0153 - binary_accuracy: 0.9993 - recall_m: 0.9962 - precision_m: 0.9998\n",
            "Train AUC: 0.9999878722724003\n",
            "Test AUC: 0.9844620141398029\n",
            "Using trainset to find threshold:\n",
            "threshold value:  0.00022608675\n",
            "f1:  0.5235949168085943\n",
            "recall:  0.9987005847368684\n",
            "precision:  0.3548054899593402\n",
            "Accuracy:  0.9636359272718545\n",
            "##################################\n",
            "Using testset to find threshold:\n",
            "threshold value:  0.00019300426\n",
            "f1:  0.05458225803811592\n",
            "recall:  0.9999500224898795\n",
            "precision:  0.02805687097457241\n",
            "Accuracy:  0.3068826137652275\n",
            "##################################\n",
            "Currently Running Epoch - 8\n",
            "1356/1356 [==============================] - 32s 24ms/step - loss: 0.0078 - binary_accuracy: 0.9994 - recall_m: 0.9967 - precision_m: 0.9999\n",
            "Train AUC: 0.9999989794406721\n",
            "Test AUC: 0.8071084791062972\n",
            "Using trainset to find threshold:\n",
            "threshold value:  0.00011637441\n",
            "f1:  0.46871580622699693\n",
            "recall:  0.5298615622969663\n",
            "precision:  0.42022235874670527\n",
            "Accuracy:  0.9759654519309039\n",
            "##################################\n",
            "Using testset to find threshold:\n",
            "threshold value:  6.980779e-05\n",
            "f1:  0.05614236912436255\n",
            "recall:  1.0\n",
            "precision:  0.028881934681128082\n",
            "Accuracy:  0.32722165444330886\n",
            "##################################\n",
            "Currently Running Epoch - 9\n",
            "1356/1356 [==============================] - 32s 24ms/step - loss: 0.0044 - binary_accuracy: 0.9994 - recall_m: 0.9966 - precision_m: 0.9997\n",
            "Train AUC: 0.9999996236678192\n",
            "Test AUC: 0.996353916630409\n",
            "Using trainset to find threshold:\n",
            "threshold value:  7.75065e-05\n",
            "f1:  0.7271205560205233\n",
            "recall:  0.9986506072267479\n",
            "precision:  0.5716819729350843\n",
            "Accuracy:  0.98500197000394\n",
            "##################################\n",
            "Using testset to find threshold:\n",
            "threshold value:  5.2050815e-05\n",
            "f1:  0.27993522414474536\n",
            "recall:  1.0\n",
            "precision:  0.16274690818289406\n",
            "Accuracy:  0.8970632941265883\n",
            "##################################\n",
            "Currently Running Epoch - 10\n",
            "1356/1356 [==============================] - 32s 24ms/step - loss: 0.0035 - binary_accuracy: 0.9995 - recall_m: 0.9971 - precision_m: 0.9999\n",
            "Train AUC: 0.9999997538744059\n",
            "Test AUC: 0.6478257687511986\n",
            "Using trainset to find threshold:\n",
            "threshold value:  8.335116e-05\n",
            "f1:  0.5723849372384937\n",
            "recall:  0.5298615622969663\n",
            "precision:  0.6223291852547547\n",
            "Accuracy:  0.9841589683179366\n",
            "##################################\n",
            "Using testset to find threshold:\n",
            "threshold value:  2.2492319e-05\n",
            "f1:  0.0503621660740694\n",
            "recall:  0.9999000449797592\n",
            "precision:  0.02583161613188107\n",
            "Accuracy:  0.24548749097498196\n",
            "##################################\n",
            "Currently Running Epoch - 11\n",
            "1356/1356 [==============================] - 32s 24ms/step - loss: 0.0030 - binary_accuracy: 0.9995 - recall_m: 0.9968 - precision_m: 0.9997\n",
            "Train AUC: 0.9999997215306602\n",
            "Test AUC: 0.5766760232546395\n",
            "Using trainset to find threshold:\n",
            "threshold value:  0.00011950228\n",
            "f1:  0.6094706196898579\n",
            "recall:  0.512669298815533\n",
            "precision:  0.7513367025562148\n",
            "Accuracy:  0.9868539737079474\n",
            "##################################\n",
            "Using testset to find threshold:\n",
            "threshold value:  2.1155933e-05\n",
            "f1:  0.04159391006452472\n",
            "recall:  1.0\n",
            "precision:  0.021238654372186482\n",
            "Accuracy:  0.07790415580831161\n",
            "##################################\n",
            "Currently Running Epoch - 12\n",
            "1356/1356 [==============================] - 32s 23ms/step - loss: 0.0027 - binary_accuracy: 0.9996 - recall_m: 0.9975 - precision_m: 0.9996\n",
            "Train AUC: 0.9999997232975975\n",
            "Test AUC: 0.9950460634751874\n",
            "Using trainset to find threshold:\n",
            "threshold value:  7.4370124e-05\n",
            "f1:  0.6066053635315336\n",
            "recall:  0.512669298815533\n",
            "precision:  0.7426875181002027\n",
            "Accuracy:  0.9866949733899468\n",
            "##################################\n",
            "Using testset to find threshold:\n",
            "threshold value:  1.9477513e-05\n",
            "f1:  0.12425403488725913\n",
            "recall:  1.0\n",
            "precision:  0.06624246417066977\n",
            "Accuracy:  0.7179514359028718\n",
            "##################################\n",
            "Currently Running Epoch - 13\n",
            "1356/1356 [==============================] - 32s 24ms/step - loss: 0.0026 - binary_accuracy: 0.9997 - recall_m: 0.9983 - precision_m: 0.9995\n",
            "Train AUC: 0.9999997278995465\n",
            "Test AUC: 0.9965312689155064\n",
            "Using trainset to find threshold:\n",
            "threshold value:  6.091644e-05\n",
            "f1:  0.6006118535649985\n",
            "recall:  0.512669298815533\n",
            "precision:  0.7249726138732817\n",
            "Accuracy:  0.9863574727149454\n",
            "##################################\n",
            "Using testset to find threshold:\n",
            "threshold value:  1.733507e-05\n",
            "f1:  0.436667212309707\n",
            "recall:  0.9999000449797592\n",
            "precision:  0.2793259430932901\n",
            "Accuracy:  0.9483788967577935\n",
            "##################################\n",
            "Currently Running Epoch - 14\n",
            "1356/1356 [==============================] - 32s 24ms/step - loss: 0.0025 - binary_accuracy: 0.9997 - recall_m: 0.9987 - precision_m: 0.9995\n",
            "Train AUC: 0.9999997192318832\n",
            "Test AUC: 0.9862522854201208\n",
            "Using trainset to find threshold:\n",
            "threshold value:  8.8305824e-05\n",
            "f1:  0.5959535809211196\n",
            "recall:  0.512669298815533\n",
            "precision:  0.7115457982173204\n",
            "Accuracy:  0.9860904721809444\n",
            "##################################\n",
            "Using testset to find threshold:\n",
            "threshold value:  1.9707082e-05\n",
            "f1:  0.09816384874593523\n",
            "recall:  0.9987505622469889\n",
            "precision:  0.051618635815336295\n",
            "Accuracy:  0.6328112656225312\n",
            "##################################\n",
            "Currently Running Epoch - 15\n",
            "1356/1356 [==============================] - 32s 24ms/step - loss: 0.0024 - binary_accuracy: 0.9997 - recall_m: 0.9990 - precision_m: 0.9996\n",
            "Train AUC: 0.9999997359210909\n",
            "Test AUC: 0.9918549206059402\n",
            "Using trainset to find threshold:\n",
            "threshold value:  8.9558904e-05\n",
            "f1:  0.6160128511162156\n",
            "recall:  0.512669298815533\n",
            "precision:  0.77153924260088\n",
            "Accuracy:  0.9872114744229489\n",
            "##################################\n",
            "Using testset to find threshold:\n",
            "threshold value:  3.5903886e-05\n",
            "f1:  0.04240277443711796\n",
            "recall:  1.0\n",
            "precision:  0.021660622462787552\n",
            "Accuracy:  0.09625719251438503\n",
            "##################################\n",
            "Currently Running Epoch - 16\n",
            "1356/1356 [==============================] - 32s 24ms/step - loss: 0.0023 - binary_accuracy: 0.9998 - recall_m: 0.9991 - precision_m: 0.9994\n",
            "Train AUC: 0.9999997308462005\n",
            "Test AUC: 0.9967710741934832\n",
            "Using trainset to find threshold:\n",
            "threshold value:  6.275699e-05\n",
            "f1:  0.6004887971784403\n",
            "recall:  0.512669298815533\n",
            "precision:  0.7246141348497157\n",
            "Accuracy:  0.9863504727009454\n",
            "##################################\n",
            "Using testset to find threshold:\n",
            "threshold value:  1.5711914e-05\n",
            "f1:  0.46259826566172296\n",
            "recall:  0.9984506971862662\n",
            "precision:  0.3010367066481828\n",
            "Accuracy:  0.9535829071658143\n",
            "##################################\n",
            "Currently Running Epoch - 17\n",
            "1356/1356 [==============================] - 33s 24ms/step - loss: 0.0023 - binary_accuracy: 0.9998 - recall_m: 0.9992 - precision_m: 0.9995\n",
            "Train AUC: 0.9999997428736821\n",
            "Test AUC: 0.9936988436770694\n",
            "Using trainset to find threshold:\n",
            "threshold value:  7.900694e-05\n",
            "f1:  0.9029276871616262\n",
            "recall:  0.9814583437453146\n",
            "precision:  0.8360331211818046\n",
            "Accuracy:  0.9957774915549831\n",
            "##################################\n",
            "Using testset to find threshold:\n",
            "threshold value:  2.7108003e-05\n",
            "f1:  0.07469235184588892\n",
            "recall:  0.9999000449797592\n",
            "precision:  0.03879517207880033\n",
            "Accuracy:  0.5042955085910171\n",
            "##################################\n",
            "Currently Running Epoch - 18\n",
            "1356/1356 [==============================] - 32s 24ms/step - loss: 0.0022 - binary_accuracy: 0.9998 - recall_m: 0.9992 - precision_m: 0.9993\n",
            "Train AUC: 0.999999759927705\n",
            "Test AUC: 0.9921360824062642\n",
            "Using trainset to find threshold:\n",
            "threshold value:  6.0641003e-05\n",
            "f1:  0.6160220994475137\n",
            "recall:  0.512669298815533\n",
            "precision:  0.7715682587438887\n",
            "Accuracy:  0.9872119744239488\n",
            "##################################\n",
            "Using testset to find threshold:\n",
            "threshold value:  1.4926684e-05\n",
            "f1:  0.06949844546611673\n",
            "recall:  0.9987505622469889\n",
            "precision:  0.03600182315077205\n",
            "Accuracy:  0.4648749297498595\n",
            "##################################\n",
            "Currently Running Epoch - 19\n",
            "1356/1356 [==============================] - 32s 24ms/step - loss: 0.0021 - binary_accuracy: 0.9998 - recall_m: 0.9992 - precision_m: 0.9994\n",
            "Train AUC: 0.9999997568298504\n",
            "Test AUC: 0.650203873867598\n",
            "Using trainset to find threshold:\n",
            "threshold value:  4.348997e-05\n",
            "f1:  0.17535693281779846\n",
            "recall:  0.5252886201209456\n",
            "precision:  0.10524550274616613\n",
            "Accuracy:  0.9011453022906046\n",
            "##################################\n",
            "Using testset to find threshold:\n",
            "threshold value:  1.7586864e-05\n",
            "f1:  0.0441660215381742\n",
            "recall:  0.9986506072267479\n",
            "precision:  0.022582371352416426\n",
            "Accuracy:  0.1351037702075404\n",
            "##################################\n",
            "Currently Running Epoch - 20\n",
            "1356/1356 [==============================] - 32s 24ms/step - loss: 0.0022 - binary_accuracy: 0.9998 - recall_m: 0.9992 - precision_m: 0.9993\n",
            "Train AUC: 0.9999997618326569\n",
            "Test AUC: 0.9718134798103656\n",
            "Using trainset to find threshold:\n",
            "threshold value:  8.9793815e-05\n",
            "f1:  0.6068435444534772\n",
            "recall:  0.5129691638762557\n",
            "precision:  0.7427723703730507\n",
            "Accuracy:  0.9867004734009468\n",
            "##################################\n",
            "Using testset to find threshold:\n",
            "threshold value:  1.836688e-05\n",
            "f1:  0.04089655631104221\n",
            "recall:  0.9987505622469889\n",
            "precision:  0.0208756839648757\n",
            "Accuracy:  0.06267112534225068\n",
            "##################################\n"
          ]
        }
      ]
    }
  ]
}